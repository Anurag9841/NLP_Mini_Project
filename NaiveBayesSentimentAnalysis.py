# -*- coding: utf-8 -*-
"""NaiveBayesSentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FYHbGywavTUqdJw3HrTx9pdnJshodCk9

# Library Imports
"""

import re
import string

import numpy as np

import nltk

"""# NLTK Downloads"""

nltk.download('twitter_samples')
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import TweetTokenizer
from nltk.stem import PorterStemmer
from nltk.corpus import twitter_samples

"""# Preprocessing Pipeline

## Data Preparation
"""

all_positive_tweets  = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

def splitTweetsForTrainTest(all_positive_tweets, all_negative_tweets, splitPercentage = 0.8):

  testSplitPos = int(splitPercentage * len(all_positive_tweets))
  testSplitNeg = int(splitPercentage * len(all_negative_tweets))

  train_pos  = all_positive_tweets[:testSplitPos]
  test_pos = all_positive_tweets[testSplitPos:]

  train_neg = all_negative_tweets[:testSplitNeg]
  test_neg = all_negative_tweets[testSplitNeg:]

  train_x = train_pos + train_neg
  test_x = test_pos + test_neg

  train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)))
  test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)))

  return train_x, test_x, train_y, test_y

train_x, test_x, train_y, test_y = splitTweetsForTrainTest(all_positive_tweets, all_negative_tweets, splitPercentage = 0.8)

"""## Preprocessing Functions"""

def process_tweet(tweet):
    """Process tweet function.
    Input:
        tweet: a string containing a tweet
    Output:
        tweets_clean: a list of words containing the processed tweet

    """
    stemmer = PorterStemmer()
    stopwords_english = stopwords.words('english')
    # remove stock market tickers like $GE
    tweet = re.sub(r'\$\w*', '', tweet)
    # remove old style retweet text "RT"
    tweet = re.sub(r'^RT[\s]+', '', tweet)
    # remove hyperlinks
    tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)
    # remove hashtags
    # only removing the hash # sign from the word
    tweet = re.sub(r'#', '', tweet)
    # tokenize tweets
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                               reduce_len=True)
    tweet_tokens = tokenizer.tokenize(tweet)

    tweets_clean = []

    for word in tweet_tokens:
        if (word not in stopwords_english and  # remove stopwords
                word not in string.punctuation):  # remove punctuation
            # tweets_clean.append(word)
            stem_word = stemmer.stem(word)  # stemming word
            tweets_clean.append(stem_word)

    return tweets_clean

# converts to {(word, sentiment score): no.of occurence), ... }
def build_freqs(tweets,ys):
  yslist=ys.tolist()
  freq={}
  vocab = set()

  for y,tweet in zip(yslist,tweets):
    for word in process_tweet(tweet):
      vocab.add(word)
      pair=(word,y)
      if pair in freq:
        freq[pair] += 1
      else:
        freq[pair] = 1
  vocab = list(vocab)
  return freq, vocab

freqs, vocab = build_freqs(train_x, train_y)

freqs

"""## Feature Extraction"""

def extract_features(tweet,freqs = freqs):
  word_l = process_tweet(tweet)
  x = np.zeros((1,3))
  x[0,0] = 1
  for word in word_l:
    x[0,1] += freqs.get((word,1.0),0)
    x[0,2] += freqs.get((word,0.0),0)
  assert(x.shape == (1,3))
  return x

X = np.zeros((len(train_x),3))
for i in range(len(train_x)):
  X[i] = extract_features(train_x[i],freqs)
y = train_y

X_test = np.zeros((len(test_x),3))
for i in range(len(test_x)):
  X_test[i] = extract_features(test_x[i],freqs)
y_test = test_y

"""# ML

## Naive Bayes [Using Jurafsky Book]
"""

# train_x = [
#     "just plain boring",
#     "entirely predictable and lacks energy",
#     "no surprises and very few laughs",
#     "very powerful",
#     "the most fun film of the summer"
# ]
# train_y = np.array([
#     0,0,0,1,1
# ])
# # freqs = build_freqs(train_x, train_y)

# D = x_train, C = [0, 1]
def trainNaiveBayes(train_x, train_y):
  D = train_x
  C = np.unique(train_y).astype(int)

  logPrior = {}
  bigDoc = {c:[] for c in C}
  freqs, vocab = build_freqs(train_x, train_y)
  N_doc = len(D)
  for c in C:
    # calculate P(c) terms
    N_c = np.unique(train_y, return_counts = True)[1][c]
    logPrior[c] = np.log((N_c/N_doc))

    bigDoc[c] = [d for d in D if (train_y[train_x.index(train_x[0])] == c)]

  logLikelihood = {}
  classSums = {}
  for c in C:
    freqsList = [freqs[(w,cl)]+1 for w, cl in freqs if cl == c]
    classSums[c] = np.array(freqsList).sum()

  for pair in freqs:
    logLikelihood[pair] = np.log((freqs[pair] + 1)/(classSums[c]+len(vocab)))

  return logPrior, logLikelihood, vocab

def testNaiveBayes(test_x, logPrior, logLikelihood, vocab = vocab, freqs = freqs, test_y = np.array([0, 1])):
  C = np.unique(test_y).astype(int)
  sum = list(logPrior.values())
  for c in C:
    # sum[c] = logPrior[c]
    tmpSum = 0
    for word in process_tweet(test_x):
      if not word in vocab:
        continue
      sum[c] += logLikelihood.get((word, c), 0)

  return np.argmax(sum)

logPrior, logLikelihood, vocab = trainNaiveBayes(train_x, train_y)

logLikelihood, logPrior

"""## Predictions"""

for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:
    print( '%s -> %f' % (tweet, testNaiveBayes(tweet, logPrior, logLikelihood)))

# calculate y_pred
y_pred = []

for tweet in test_x:
  y = testNaiveBayes(tweet, logPrior, logLikelihood)
  y_pred.append(y)

"""## Testing"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

metrics = confusion_matrix(test_y, y_pred, labels = [0,1])

metrics

disp = ConfusionMatrixDisplay(confusion_matrix = metrics, display_labels = [0, 1])
disp.plot()
plt.show()

tn, fp, fn, tp = metrics.reshape(metrics.size)

dict(
    precision = tp/(tp + fn),
  recall = tp / (tp + fp),
  accuracy = (tp + tn) / (tp + fp + tn + fn))